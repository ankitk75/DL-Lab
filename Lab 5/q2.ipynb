{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f3b3f62-4427-4ba6-83aa-4463183398e7",
   "metadata": {},
   "source": [
    "2. Apply torch.nn.Conv2d to the input image of Qn 1 with out-channel=3 and observe the\n",
    "output. Implement the equivalent of torch.nn.Conv2d using the torch.nn.functional.conv2D\n",
    "to get the same output. You may ignore bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4024bb5-9f14-4b40-b90c-68d21cec30bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image= tensor([[0.6853, 0.9800, 0.4993, 0.5879, 0.6278, 0.5285],\n",
      "        [0.3428, 0.6774, 0.4936, 0.0942, 0.2988, 0.7249],\n",
      "        [0.6831, 0.5740, 0.8701, 0.2059, 0.0838, 0.4373],\n",
      "        [0.8492, 0.1970, 0.8978, 0.4885, 0.4745, 0.5264],\n",
      "        [0.5303, 0.7252, 0.5083, 0.4456, 0.1050, 0.2693],\n",
      "        [0.2950, 0.8138, 0.2319, 0.4564, 0.4434, 0.2933]])\n",
      "image.shape= torch.Size([1, 6, 6])\n",
      "image.shape= torch.Size([1, 1, 6, 6])\n",
      "image= tensor([[[[0.6853, 0.9800, 0.4993, 0.5879, 0.6278, 0.5285],\n",
      "          [0.3428, 0.6774, 0.4936, 0.0942, 0.2988, 0.7249],\n",
      "          [0.6831, 0.5740, 0.8701, 0.2059, 0.0838, 0.4373],\n",
      "          [0.8492, 0.1970, 0.8978, 0.4885, 0.4745, 0.5264],\n",
      "          [0.5303, 0.7252, 0.5083, 0.4456, 0.1050, 0.2693],\n",
      "          [0.2950, 0.8138, 0.2319, 0.4564, 0.4434, 0.2933]]]])\n",
      "kernel= tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "outimage= tensor([[[[2.6855, 3.6785, 3.3324, 2.6016, 2.8622, 2.1801],\n",
      "          [3.9426, 5.8056, 4.9824, 3.7615, 3.5892, 2.7012],\n",
      "          [3.3235, 5.5850, 4.4985, 3.9073, 3.3343, 2.5457],\n",
      "          [3.5588, 5.8351, 4.9125, 4.0798, 3.0363, 1.8963],\n",
      "          [3.4105, 5.0486, 4.7646, 4.0517, 3.5025, 2.1119],\n",
      "          [2.3643, 3.1045, 3.1813, 2.1908, 2.0131, 1.1111]]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "image = torch.rand(6,6)\n",
    "print(\"image=\", image)\n",
    "\n",
    "#Add a new dimension along 0th dimension\n",
    "#i.e. (6,6) becomes (1,6,6). This is because\n",
    "#pytorch expects the input to conv2D as 4d tensor\n",
    "image = image.unsqueeze(dim=0)\n",
    "print(\"image.shape=\", image.shape)\n",
    "\n",
    "image = image.unsqueeze(dim=0)\n",
    "print(\"image.shape=\", image.shape)\n",
    "\n",
    "print(\"image=\", image)\n",
    "\n",
    "kernel = torch.ones(3,3)\n",
    "#kernel = torch.rand(3,3)\n",
    "print(\"kernel=\", kernel)\n",
    "\n",
    "kernel = kernel.unsqueeze(dim=0)\n",
    "kernel = kernel.unsqueeze(dim=0)\n",
    "\n",
    "#Perform the convolution\n",
    "outimage = F.conv2d(image, kernel, stride=1, padding=1)\n",
    "print(\"outimage=\", outimage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e150f07-f3eb-4e53-b906-846cd8ba4530",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(1, 3, kernel_size=(3, 3), stride=(1, 1))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = nn.Conv2d(1, 3, kernel_size=3, stride=1)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae5476d-285a-46bb-8080-b09b77fd7f0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
