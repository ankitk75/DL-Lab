{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e69bcf8-dd62-4ad4-a92f-1158c48488ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/1000], Loss: 391.9741\n",
      "Epoch [100/1000], Loss: 29.8055\n",
      "Epoch [200/1000], Loss: 13.3521\n",
      "Epoch [300/1000], Loss: 8.5025\n",
      "Epoch [400/1000], Loss: 6.2372\n",
      "Epoch [500/1000], Loss: 4.9217\n",
      "Epoch [600/1000], Loss: 4.0907\n",
      "Epoch [700/1000], Loss: 3.4471\n",
      "Epoch [800/1000], Loss: 2.9678\n",
      "Epoch [900/1000], Loss: 2.5898\n",
      "orem ipsum dolor sit amet, consectetur adipiscing elit. Sed do eiusmod tempor incididunt ut labore e"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import string\n",
    "\n",
    "# Define the RNN model\n",
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(CharRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.rnn = nn.RNN(hidden_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, hidden = self.rnn(x, hidden)\n",
    "        output = self.fc(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(1, batch_size, self.hidden_size).to(device)\n",
    "\n",
    "# Function to convert character to tensor and vice versa\n",
    "def char_to_tensor(char):\n",
    "    all_letters = string.ascii_letters + \" .,;'\"\n",
    "    return torch.tensor([all_letters.find(char)], dtype=torch.long).to(device)\n",
    "\n",
    "def tensor_to_char(output):\n",
    "    all_letters = string.ascii_letters + \" .,;'\"\n",
    "    _, topi = output.topk(1)\n",
    "    return all_letters[topi.item()]\n",
    "\n",
    "# Define training parameters\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "input_size = len(string.ascii_letters + \" .,;'\")\n",
    "hidden_size = 128\n",
    "output_size = input_size\n",
    "seq_len = 100\n",
    "num_epochs = 1000\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Initialize the model and move it to the device\n",
    "model = CharRNN(input_size, hidden_size, output_size).to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training data\n",
    "text = \"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\"\n",
    "text_len = len(text)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for i in range(0, text_len - seq_len, seq_len):\n",
    "        inputs = torch.cat([char_to_tensor(char) for char in text[i:i+seq_len]]).view(1, -1)\n",
    "        targets = torch.cat([char_to_tensor(char) for char in text[i+1:i+seq_len+1]]).view(1, -1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        hidden = model.init_hidden(1)\n",
    "\n",
    "        loss = 0\n",
    "        for j in range(seq_len):\n",
    "            output, hidden = model(inputs[:,j].unsqueeze(0), hidden)\n",
    "            loss += criterion(output.view(1, -1), targets[:,j])\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch [{epoch}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Example usage for text generation\n",
    "start_char = 'L'\n",
    "num_predict = 100\n",
    "\n",
    "with torch.no_grad():\n",
    "    input_char = start_char\n",
    "    hidden = model.init_hidden(1)\n",
    "\n",
    "    for _ in range(num_predict):\n",
    "        input_tensor = char_to_tensor(input_char)\n",
    "        output, hidden = model(input_tensor.unsqueeze(0), hidden)\n",
    "        predicted_char = tensor_to_char(output.squeeze(0))\n",
    "        print(predicted_char, end='')\n",
    "        input_char = predicted_char\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfc57396-9cf7-4ff8-9c30-b6d15e33ce63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/1000], Loss: 391.0338\n",
      "Epoch [100/1000], Loss: 28.4291\n",
      "Epoch [200/1000], Loss: 12.1416\n",
      "Epoch [300/1000], Loss: 7.6467\n",
      "Epoch [400/1000], Loss: 5.5639\n",
      "Epoch [500/1000], Loss: 4.3399\n",
      "Epoch [600/1000], Loss: 3.5314\n",
      "Epoch [700/1000], Loss: 2.9532\n",
      "Epoch [800/1000], Loss: 2.5160\n",
      "Epoch [900/1000], Loss: 2.1725\n",
      "orem ipsum dolor sit amet, consectetur adipiscing elit. Sed do eiusmod tempor incididunt ut labore e"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import string\n",
    "\n",
    "# Define the RNN model\n",
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(CharRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.rnn = nn.RNN(hidden_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, hidden = self.rnn(x, hidden)\n",
    "        output = self.fc(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(1, batch_size, self.hidden_size).to(device)\n",
    "\n",
    "# Define training parameters\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "input_size = len(string.ascii_letters + \" .,;'\")\n",
    "hidden_size = 128\n",
    "output_size = input_size\n",
    "seq_len = 100\n",
    "num_epochs = 1000\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Initialize the model and move it to the device\n",
    "model = CharRNN(input_size, hidden_size, output_size).to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training data\n",
    "text = \"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\"\n",
    "text_len = len(text)\n",
    "\n",
    "# Convert characters to indices and vice versa\n",
    "all_letters = string.ascii_letters + \" .,;'\"\n",
    "char_to_index = {char: i for i, char in enumerate(all_letters)}\n",
    "index_to_char = {i: char for i, char in enumerate(all_letters)}\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for i in range(0, text_len - seq_len, seq_len):\n",
    "        inputs = torch.tensor([char_to_index[char] for char in text[i:i+seq_len]]).view(1, -1).to(device)\n",
    "        targets = torch.tensor([char_to_index[char] for char in text[i+1:i+seq_len+1]]).view(1, -1).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        hidden = model.init_hidden(1)\n",
    "\n",
    "        loss = 0\n",
    "        for j in range(seq_len):\n",
    "            output, hidden = model(inputs[:,j].unsqueeze(0), hidden)\n",
    "            loss += criterion(output.view(1, -1), targets[:,j])\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch [{epoch}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Example usage for text generation\n",
    "start_char = 'L'\n",
    "num_predict = 100\n",
    "\n",
    "with torch.no_grad():\n",
    "    input_char = start_char\n",
    "    hidden = model.init_hidden(1)\n",
    "\n",
    "    for _ in range(num_predict):\n",
    "        input_tensor = torch.tensor([char_to_index[input_char]]).to(device)\n",
    "        output, hidden = model(input_tensor.unsqueeze(0), hidden)\n",
    "        predicted_index = output.argmax().item()\n",
    "        predicted_char = index_to_char[predicted_index]\n",
    "        print(predicted_char, end='')\n",
    "        input_char = predicted_char\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693e74f7-9387-461b-b552-9bbe834f1501",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
